---
title: "Practical Machine Learning Project Report"
author: "Ananth"
date: "Thursday, December 18, 2014"
output: html_document
---

###Executive Summary

The main objective of this project is to demonstrate the analysis and model building process of machine learning tools and techniques. 

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.

The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. They made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg)

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. they perform an exercise with a barbell using five techniques that range "perfect form" (A) to "very poor form" (E).

The question these researchers are exploring is whether machine learning techniques can be used on the data they collected to predict whether a person is using proper form in their exercise or not. The ultimate vision of the team is to provide real-time feedback to those performing exercises to improve the quality of their exercise experience.

###Data Sources

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

###Load required Libraries and set default Project directory

```{r}

library(caret)
library("randomForest")
library(gbm)
library(plyr)
setwd("C:/Users/admin/Desktop/Coursera/Practical Machine Learning/Project")
```


###Load Training and Testing Dataset

```{r}
training <- read.csv("pml-training.csv", row.names = 1)
testing <- read.csv("pml-testing.csv", row.names = 1)

dim(training)
dim(testing)

```


The project source data consists of two data sets. "pml-training.csv"" contains 19622 
observations with 159 variables. "pml-testing.csv"" contains 20 observations and 159 variables.


###Data Cleansing and Preparation

Remove near zero covariates and those with more than 80% missing values since these variables will not provide much power for prediction.


```{r}
# remove near zero covariates
zerovar <- nearZeroVar(training, saveMetrics = T)
training <- training[, !zerovar$nzv]

# remove variables with more than 80% missing values
mval <- sapply(colnames(training), function(x) if(sum(is.na(training[, x])) > 0.8*nrow(training)){return(T)}else{return(F)})
training <- training[, !mval]

```

###Model Development

1. Calculate correlations between each remaining feature to the response, "classe". Use spearman rank based correlation because classe is a factor.

2. Plot the two features that have highest correlation with "classe"" and color with classe to see if we can separate response based on these features.

```{r}
# calculate correlations
cor <- abs(sapply(colnames(training[, -ncol(training)]), function(x) cor(as.numeric(training[, x]), as.numeric(training$classe), method = "spearman")))
summary(cor)

#plot predictors

plot(training[, names(which.max(cor))], training[, names(which.max(cor[-which.max(cor)]))], col = training$classe, pch = 19, cex = 0.1, xlab = names(which.max(cor)), ylab = names(which.max(cor[-which.max(cor)])))

```
It appears like no strong predictors that correlates with "classe" well, so linear regression model is probably not suitable in this case. Boosting and random forests algorithms may generate more robust predictions for our data.

###Boosting model

Fit model with boosting algorithm and 10-fold cross validation to predict "classe" with all other predictors.

Plot accuracy of this model on the scale [0.9, 1].

```{r}

set.seed(456)
boostFit <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))
boostFit
plot(boostFit, ylim = c(0.9, 1))

```
The boosting algorithm generated a good model with accuracy = 0.997.

###Random forests model

Fit model with random forests algorithm and 10-fold cross validation to predict classe with all other predictors. Plot accuracy of the model on the same scale as boosting model.

```{r}

set.seed(456)
rfFit <- train(classe ~ ., method = "rf", data = training, importance = T, trControl = trainControl(method = "cv", number = 10))
rfFit
plot(rfFit, ylim = c(0.9, 1))
imp <- varImp(rfFit)$importance
imp$max <- apply(imp, 1, max)
imp <- imp[order(imp$max, decreasing = T), ]

```

The random forests algorithm generated a very accurate model with accuracy close to 1. Compared to boosting model, this model generally has better performance in terms of accuracy as we see from the plots.

###Final model and prediction

When we Compare the model accuracy of the two models generated, random forests and boosting, random forests model has overall better accuracy.

The final random forests model contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are r rownames(imp)[1:5].
Estimated out of sample error rate for the random forests model is 0.04% as reported by the final model.

Predict the test set and output results for automatic grader.

```{r}

# final model
rfFit$finalModel
# prediction
(prediction <- as.character(predict(rfFit, testing)))

```

###Generate Files
```{r}

# code from Prediction Assignment Submission: Instructions
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./problem_id_", i, ".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}
pml_write_files(prediction)

```


